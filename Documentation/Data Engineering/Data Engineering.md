# Data Engineering Journey 🚀🔧🔍

Welcome to our fun and formal data engineering journey! 🎉 Let's dive into the exciting world of data processing, analysis, and optimization with some cool emojis along the way! 🌍🔍

## Step 1: Exploratory Data Analysis (EDA) 📊🔬

Our data engineering adventure began with an EDA performed using pandas profiling on two APIs: NOAA and USGS. This initial exploration helped us gain valuable insights into the data's structure and characteristics.

## Step 2: CSV Testing and Python Functions 🐍💻

Once equipped with insights from EDA, we embarked on the next phase. Some of our amazing team members began crafting Python functions in the ETL folder to efficiently process and manipulate the data. Meanwhile, another group focused on analyzing and designing the optimal data structure to be implemented on the mighty GCP (Google Cloud Platform).

## Step 3: Building the ETL Pipeline 🚀🌪️

With the pieces falling into place, it was time to unite our efforts. We decided to create a robust ETL pipeline that would transform the data and prepare it for consumption before storing it in BigQuery. Excitingly, this ETL magic would happen automatically every week using <b>Airflow</b>, and we even set up a backup plan! Data security and availability are our top priorities. So, we replicated the entire structure on two separate GCP servers, ensuring data continuity in case of technical glitches.

## Step 4: Easy Data Consumption in BigQuery 🕵️‍♂️💡

Now that the data is ready and waiting, it's easily consumable through simple SQL queries in BigQuery. Let the data exploration and analysis begin!

And that's our thrilling data engineering journey! We hope you enjoyed this ride with us. If you have any questions or want to explore the data further, feel free to jump in! 🌟🚀🔧

Stay tuned for more exciting data adventures in the future! 🎉🌈📊